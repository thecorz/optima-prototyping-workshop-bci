{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed371ee7-4e8d-412e-8f59-24bb4360509f",
   "metadata": {},
   "source": [
    "![Image Description](data:image/png;base64,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) \n",
    "![Image Description](data:image/png;base64,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)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdef63-6c5a-41c7-89a9-6cee1750d653",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin: 20px; background-color: #f9f9f9; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"font-family: Arial, sans-serif; color: #333;\">OPTIMA Prototyping Workshop, Berlin</h1>\n",
    "    <a href=\"#\" style=\"text-decoration: none; padding: 10px 20px; background-color: #4CAF50; color: white; border-radius: 5px; font-weight: bold; transition: background-color 0.3s;\">\n",
    "        November 6-8, 2024\n",
    "    </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ef4be-4e06-492c-9dc9-69d9a4dce8bb",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center; color: #2E3A4D;\">Breast Tumor Molecular Subtype Prediction (Luminal A/B, HER2 and Triple-negative) in Mammograms using Deep Learning</h2>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 1.1em; margin-bottom: 20px;\">\n",
    "    <strong>Centre for Biomarkers and Biotherapeutics</strong><br>\n",
    "    <strong>Barts Cancer Institute, Queen Mary University of London, UK</strong>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6842d-ad9a-4819-bee0-9f3eaedf07e0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 1.1em; background-color: #f4f4f4; padding: 15px; border-radius: 8px;\">\n",
    "    <p><strong>Author:</strong> Dr. Vivek Kumar Singh (Group Leader in AI & Imaging)</p>\n",
    "    <p><strong>Copyright:</strong> 2024, Tumor classification</p>\n",
    "    <p><strong>Maintainer:</strong> Dr. Vivek Kumar Singh</p>\n",
    "    <p><strong>Credits:</strong> Dr. Vivek Kumar Singh</p>\n",
    "    <p><strong>Email:</strong> <a href=\"mailto:vivek.singh@qmul.ac.uk\" style=\"color: #007BFF; text-decoration: none;\">vivek.singh@qmul.ac.uk</a></p>\n",
    "    <p><strong>Status:</strong> Development</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473e8d2-b728-4254-80e2-d8d1ca5d9245",
   "metadata": {},
   "source": [
    "## <span style=\"color:black\">Overview</span>\n",
    "This Jupyter notebook implements a pipeline to classify breast molecular subtypes using deep learning techniques on mammogram images.\n",
    "\n",
    "## Dataset\n",
    "- The dataset can be accessed here: [Cancer Imaging Archive - CMMD Collection](https://www.cancerimagingarchive.net/collection/cmmd/)\n",
    "\n",
    "## <span style=\"color:black\">Pipeline Steps</span>\n",
    "\n",
    "1. **Import Libraries:** Essential libraries like PyTorch, NumPy, and Matplotlib are imported.\n",
    "\n",
    "2. **Data Preparation:**\n",
    "   - **Load Dataset:** Mammogram images and labels are loaded.\n",
    "   - **Data Augmentation:** Techniques like rotation and flipping enhance the dataset.\n",
    "   - **Data Splitting:** The dataset is divided into training, validation, and test sets.\n",
    "\n",
    "3. **Model Development:**\n",
    "   - **Model Architecture:** A Convolutional Neural Network (CNN) is defined.\n",
    "   - **Loss Function and Optimizer:** CrossEntropyLoss and Adam optimizer are selected.\n",
    "\n",
    "4. **Model Training:** The model is trained on the training set, with metrics recorded for validation.\n",
    "\n",
    "5. **Model Evaluation:**\n",
    "   - **Testing:** The model's performance is assessed on the test set.\n",
    "   - **Confusion Matrix & ROC Curve:** Visualizations are generated to evaluate classification performance.\n",
    "\n",
    "6. **Results Visualization:** Plots of accuracy, loss, confusion matrix, and ROC curves are provided for analysis.\n",
    "\n",
    "7. **Conclusion:** Summary of findings and insights into model effectiveness in classifying breast cancer molecular subtypes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5423a5-3e7a-47a5-a6a9-9195879d0ad3",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "- **Start by importing libraries required for the project**:\n",
    "  - **PyTorch**: Main framework for building and training deep learning models.\n",
    "\n",
    "  - **Visualization Libraries**: \n",
    "    - Enable plotting of data distributions, model performance metrics, and other insights.\n",
    "\n",
    "- **Benefits of Importing Libraries Early**:\n",
    "  - Streamlines workflow.\n",
    "  - Reduces interruptions during critical development phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfdf06-1a51-4b12-8a1e-853c90d71669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time, copy, argparse\n",
    "import multiprocessing\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "import timm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import cv2\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d3b74-cb3b-475d-aa36-ffbe06f0c8fb",
   "metadata": {},
   "source": [
    "# Data split into training, validation and testing\n",
    "The dataset is divided into three distinct subsets: training, validation, and testing. This approach is essential to ensure that the model is trained effectively and evaluated in a robust manner.\n",
    "\n",
    "- **Training Set**\n",
    "  - **Purpose**: Used to train the model by identifying patterns and adjusting parameters.\n",
    "  - **Process**: Model learns relationships within the data through optimization of loss functions.\n",
    "\n",
    "\n",
    "- **Validation Set**\n",
    "  - **Purpose**: Fine-tunes model hyperparameters (e.g., learning rate, batch size).\n",
    "  - **Process**: Evaluates model performance after each epoch, helping to prevent overfitting by ensuring the model doesn't become overly specialized to the training data.\n",
    "\n",
    "\n",
    "- **Testing Set**\n",
    "  - **Purpose**: Final evaluation on unseen data to measure generalization.\n",
    "  - **Process**: Provides an unbiased estimate of model performance on real-world data, confirming its accuracy on new predictions.\n",
    "\n",
    "**Why This Split Matters**: Ensures the model is well-trained, fine-tuned, and rigorously tested, allowing it to generalize and make accurate predictions on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39fcc-9d24-4c78-9340-3aed40926b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure(source_dir, dest_dir, binary=False):\n",
    "    split_root = os.path.join(dest_dir, 'split_binary' if binary else 'split_multiclass')\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = os.path.join(split_root, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        if binary:\n",
    "            os.makedirs(os.path.join(split_dir, 'Luminal'), exist_ok=True)\n",
    "            os.makedirs(os.path.join(split_dir, 'Non_Luminal'), exist_ok=True)\n",
    "        else:\n",
    "            for class_name in os.listdir(source_dir):\n",
    "                if os.path.isdir(os.path.join(source_dir, class_name)):\n",
    "                    os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
    "    logging.info(f\"Created directory structure in {split_root}\")\n",
    "\n",
    "def group_images_by_patient(source_dir):\n",
    "    patient_images = defaultdict(list)\n",
    "    class_counts = defaultdict(int)\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for image_name in os.listdir(class_path):\n",
    "            if os.path.isfile(os.path.join(class_path, image_name)):\n",
    "                parts = image_name.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    patient_id = parts[1]\n",
    "                else:\n",
    "                    patient_id = image_name.split('.')[0]\n",
    "                patient_images[patient_id].append((class_name, image_name))\n",
    "                class_counts[class_name] += 1\n",
    "    \n",
    "    if not patient_images:\n",
    "        logging.warning(f\"No images found in {source_dir}\")\n",
    "    else:\n",
    "        logging.info(f\"Found {len(patient_images)} patients\")\n",
    "        logging.info(f\"Class counts: {dict(class_counts)}\")\n",
    "    \n",
    "    return patient_images\n",
    "\n",
    "def split_patients(patient_ids, train_ratio=0.7, val_ratio=0.2):\n",
    "    random.shuffle(patient_ids)\n",
    "    total_patients = len(patient_ids)\n",
    "    train_split = int(train_ratio * total_patients)\n",
    "    val_split = int(val_ratio * total_patients)\n",
    "\n",
    "    train_patients = patient_ids[:train_split]\n",
    "    val_patients = patient_ids[train_split:train_split+val_split]\n",
    "    test_patients = patient_ids[train_split+val_split:]\n",
    "    \n",
    "    logging.info(f\"Split: {len(train_patients)} train, {len(val_patients)} val, {len(test_patients)} test\")\n",
    "    \n",
    "    return train_patients, val_patients, test_patients\n",
    "\n",
    "def copy_images(patient_list, split_name, patient_images, source_dir, dest_dir, binary=False):\n",
    "    split_root = os.path.join(dest_dir, 'split_binary' if binary else 'split_multiclass')\n",
    "    class_counts = defaultdict(int)\n",
    "    for patient_id in patient_list:\n",
    "        for class_name, image_name in patient_images[patient_id]:\n",
    "            src = os.path.join(source_dir, class_name, image_name)\n",
    "            if binary:\n",
    "                if class_name in ['LuminalA', 'LuminalB']:\n",
    "                    dst_class = 'Luminal'\n",
    "                else:\n",
    "                    dst_class = 'Non_Luminal'\n",
    "            else:\n",
    "                dst_class = class_name\n",
    "            dst = os.path.join(split_root, split_name, dst_class, image_name)\n",
    "            if os.path.isfile(src):\n",
    "                os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "                shutil.copy2(src, dst)\n",
    "                class_counts[dst_class] += 1\n",
    "            else:\n",
    "                logging.warning(f\"Skipping non-file: {src}\")\n",
    "    logging.info(f\"Copied images for {len(patient_list)} patients to {split_name} split ({'binary' if binary else 'multiclass'})\")\n",
    "    logging.info(f\"Class counts for {split_name} ({'binary' if binary else 'multiclass'}): {dict(class_counts)}\")\n",
    "\n",
    "def verify_split(dest_dir):\n",
    "    for split_type in ['split_multiclass', 'split_binary']:\n",
    "        split_root = os.path.join(dest_dir, split_type)\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_dir = os.path.join(split_root, split)\n",
    "            class_counts = defaultdict(int)\n",
    "            for class_name in os.listdir(split_dir):\n",
    "                class_dir = os.path.join(split_dir, class_name)\n",
    "                if os.path.isdir(class_dir):\n",
    "                    class_counts[class_name] = len(os.listdir(class_dir))\n",
    "            logging.info(f\"{split_type.capitalize()} - {split.capitalize()} split counts: {dict(class_counts)}\")\n",
    "\n",
    "def split_dataset(source_dir, dest_dir):\n",
    "    if not os.path.exists(source_dir):\n",
    "        raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n",
    "    \n",
    "    split_multiclass = os.path.join(dest_dir, 'split_multiclass')\n",
    "    split_binary = os.path.join(dest_dir, 'split_binary')\n",
    "    \n",
    "    if os.path.exists(split_multiclass) and os.path.exists(split_binary):\n",
    "        logging.info(\"Split folders already exist. Skipping splitting process.\")\n",
    "        verify_split(dest_dir)\n",
    "        return\n",
    "    \n",
    "    # Create destination directory if it doesn't exist\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    # Create directory structures\n",
    "    create_directory_structure(source_dir, dest_dir, binary=False)\n",
    "    create_directory_structure(source_dir, dest_dir, binary=True)\n",
    "    \n",
    "    patient_images = group_images_by_patient(source_dir)\n",
    "    if not patient_images:\n",
    "        raise ValueError(\"No images found to split\")\n",
    "    \n",
    "    patient_ids = list(patient_images.keys())\n",
    "    \n",
    "    train_patients, val_patients, test_patients = split_patients(patient_ids)\n",
    "    \n",
    "    # Multiclass split\n",
    "    copy_images(train_patients, 'train', patient_images, source_dir, dest_dir, binary=False)\n",
    "    copy_images(val_patients, 'val', patient_images, source_dir, dest_dir, binary=False)\n",
    "    copy_images(test_patients, 'test', patient_images, source_dir, dest_dir, binary=False)\n",
    "    \n",
    "    # Binary split\n",
    "    copy_images(train_patients, 'train', patient_images, source_dir, dest_dir, binary=True)\n",
    "    copy_images(val_patients, 'val', patient_images, source_dir, dest_dir, binary=True)\n",
    "    copy_images(test_patients, 'test', patient_images, source_dir, dest_dir, binary=True)\n",
    "    \n",
    "    logging.info(\"Dataset split complete for both multiclass and binary classifications!\")\n",
    "    \n",
    "    # Verify the split\n",
    "    verify_split(dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963ea7f-10c1-4c80-9edf-daa34346cd1f",
   "metadata": {},
   "source": [
    "# Organizing the Dataset for Training, Validation, and Testing with Class Subfolders\n",
    "\n",
    "1. **Define a Root Folder**:\n",
    "   - Create a main directory (e.g., `dataset_root`) where all split images will be stored.\n",
    "\n",
    "2. **Create Subfolders for Each Split**:\n",
    "   - Inside the root folder, create three subdirectories:\n",
    "     - `train`: Stores images for training the model.\n",
    "     - `validation`: Contains images for fine-tuning the model’s hyperparameters.\n",
    "     - `test`: Holds images reserved for final evaluation of the model.\n",
    "\n",
    "3. **Create Class Subfolders Within Each Split**:\n",
    "   - In each of the `train`, `validation`, and `test` subdirectories, create two subfolders:\n",
    "     - `Luminal`: Stores images labeled as Luminal A & B.\n",
    "     - `Non_Luminal`: Stores images labeled as HER2-enriched & Triple-Negative.\n",
    "\n",
    "4. **Save Split Images in Class Folders**:\n",
    "   - After splitting the dataset, save each image in its corresponding class folder within the appropriate split folder to ensure organization and accessibility.\n",
    "\n",
    "**Example Directory Structure**:\n",
    "```plaintext\n",
    "dataset_root/\n",
    "├── train/\n",
    "│   ├── Luminal/\n",
    "│   │   ├── image1.png\n",
    "│   │   ├── image2.png\n",
    "│   │   └── ...\n",
    "│   └── Non_Luminal/\n",
    "│       ├── image1.png\n",
    "│       ├── image2.png\n",
    "│       └── ...\n",
    "├── validation/\n",
    "│   ├── Luminal/\n",
    "│   │   ├── image1.png\n",
    "│   │   ├── image2.png\n",
    "│   │   └── ...\n",
    "│   └── Non_Luminal/\n",
    "│       ├── image1.png\n",
    "│       ├── image2.png\n",
    "│       └── ...\n",
    "└── test/\n",
    "    ├── Luminal/\n",
    "    │   ├── image1.png\n",
    "    │   ├── image2.png\n",
    "    │   └── ...\n",
    "    └── Non_Luminal/\n",
    "        ├── image1.png\n",
    "        ├── image2.png\n",
    "        └── ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f6603-1657-4fa2-beb8-fdf80c6670c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/vs169897/prototyping_202411/team_AI_1/Radiology/Phase_II/\"\n",
    "source_directory = os.path.join(root, 'CMMD_Molecular_dataset', 'ROI')\n",
    "destination_directory = os.path.join(root, \"CMMD_Molecular_dataset\")\n",
    "split_dataset(source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eedb92-eee7-4d06-b5eb-e034d41efaef",
   "metadata": {},
   "source": [
    "# Setting Up Directory Paths for Training, Validation, and Checkpoints\n",
    "\n",
    "- **Define Paths**:\n",
    "  - **train_directory** and **valid_directory**: Paths for the training and validation datasets. Use `os.path.join` to ensure compatibility across operating systems.\n",
    "  - **checkpoint_directory**: Path to save model checkpoints. The command `os.makedirs(checkpoint_directory, exist_ok=True)` creates the directory if it doesn’t exist.\n",
    "\n",
    "- **Model Save Path**:\n",
    "  - **model_save_path**: Sets the location for saving model weights or checkpoints (e.g., `model.pth`).\n",
    "\n",
    "- **Confirmation**:\n",
    "  - After running this setup, the paths for training, validation, and checkpoint directories will be displayed to confirm they are correctly set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264e72c-67da-4470-a07c-282be8f8ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def select_training_mode():\n",
    "    while True:\n",
    "        choice = input(\"Select training mode (2 for binary, 4 for multiclass): \").strip()\n",
    "        if choice in ['2', '4']:\n",
    "            return choice\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 2 or 4.\")\n",
    "\n",
    "def setup_directories(root, mode):\n",
    "    if mode == '2':\n",
    "        split_folders = ['split_binary', 'split']\n",
    "    else:\n",
    "        split_folders = ['split_multiclass', 'split']\n",
    "\n",
    "    train_directory = None\n",
    "    valid_directory = None\n",
    "    test_directory = None\n",
    "\n",
    "    for split_folder in split_folders:\n",
    "        potential_train = os.path.join(root, 'CMMD_Molecular_dataset', split_folder, 'train')\n",
    "        potential_valid = os.path.join(root, 'CMMD_Molecular_dataset', split_folder, 'val')\n",
    "        potential_test = os.path.join(root, 'CMMD_Molecular_dataset', split_folder, 'test')\n",
    "        \n",
    "        if os.path.exists(potential_train) and os.path.exists(potential_valid) and os.path.exists(potential_test):\n",
    "            train_directory = potential_train\n",
    "            valid_directory = potential_valid\n",
    "            test_directory = potential_test\n",
    "            break\n",
    "\n",
    "    if train_directory is None or valid_directory is None or test_directory is None:\n",
    "        raise FileNotFoundError(f\"Could not find appropriate split folder for mode {mode}\")\n",
    "\n",
    "    # Set the checkpoint directory path\n",
    "    checkpoint_directory = os.path.join(root, f'checkpoint_{split_folder}')\n",
    "\n",
    "    # Create the checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_directory, exist_ok=True)\n",
    "\n",
    "    # Set the model save path\n",
    "    model_save_path = os.path.join(checkpoint_directory, 'model.pth')\n",
    "\n",
    "    return train_directory, valid_directory, test_directory, checkpoint_directory, model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3738d92-d211-40ab-a47e-443ab54c8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = \"/media/analysis-user/Data-Shared/EU_Optima/Final_Workshop/Phase_II/\"\n",
    "\n",
    "# Let the user choose the training mode\n",
    "mode = select_training_mode()\n",
    "\n",
    "try:\n",
    "    # Setup directories based on the chosen mode\n",
    "    train_directory, valid_directory, test_directory, checkpoint_directory, model_save_path = setup_directories(root, mode)\n",
    "\n",
    "    print(f\"Training directory: {train_directory}\")\n",
    "    print(f\"Validation directory: {valid_directory}\")\n",
    "    print(f\"Test directory: {test_directory}\")\n",
    "    print(f\"Checkpoint directory created at: {checkpoint_directory}\")\n",
    "    print(f\"Model will be saved at: {model_save_path}\")\n",
    "\n",
    "    # Set up evaluation directory and model path\n",
    "    EVAL_DIR = test_directory\n",
    "    EVAL_MODEL = model_save_path\n",
    "    print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "    print(f\"Evaluation model path: {EVAL_MODEL}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please check your directory structure and ensure the split folders exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18529e2-5abd-4d9a-93b9-239d46d1fa28",
   "metadata": {},
   "source": [
    "# Model hyperparameters\n",
    "\n",
    "Key hyperparameters define the training behavior, affecting model performance, speed, and accuracy:\n",
    "\n",
    "- **Batch Size (bs)**:\n",
    "  - Determines the number of samples processed before updating weights.\n",
    "  - Smaller batch sizes (e.g., 32) lead to more frequent updates, while larger ones make better use of GPU memory but may affect convergence.\n",
    "\n",
    "- **Number of Epochs (num_epochs)**:\n",
    "  - Defines the total passes through the dataset.\n",
    "  - Higher values provide more learning opportunities but risk overfitting if too high.\n",
    "\n",
    "- **Number of Classes (num_classes)**:\n",
    "  - Total output categories to predict (e.g., 2 for binary classification like benign vs. malignant).\n",
    "\n",
    "\n",
    "- **Return Preactivation (RETURN_PREACTIVATION)**:\n",
    "  - Determines if the model outputs pre-activation values (default: `False`).\n",
    "  - Useful for specific analyses but typically unnecessary for general training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c80a1-2d6b-4b52-bfb1-71d5d211c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 4 \n",
    "# Number of epochs\n",
    "num_epochs = 40\n",
    "# Number of classes\n",
    "num_classes = 2\n",
    "\n",
    "RETURN_PREACTIVATION = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba65f5-eeff-470e-a75d-66a45038df41",
   "metadata": {},
   "source": [
    "# Apply Transformations to the Data\n",
    "\n",
    "Define data augmentation and normalization techniques to enhance model generalization:\n",
    "\n",
    "- **Data Augmentation** (for training data):\n",
    "  - Introduces variations in the images, helping the model generalize better.\n",
    "  - Common techniques include random rotations, flips, and scaling.\n",
    " \n",
    "- **Tensor**:\n",
    "\n",
    "    - `transforms.ToTensor()` is a transformation provided by the PyTorch library, specifically within the `torchvision.transforms` module.\n",
    "    - Converts a PIL Image or a NumPy ndarray (representing an image) into a PyTorch tensor, which is required for model training and inference.\n",
    "\n",
    "- **Normalization**:\n",
    "  - Adjusts pixel values to a consistent range, aiding model stability and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaef083-5c54-4e8e-ad1c-ad2526a3ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying transforms to the data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        # transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff9234-1a41-49ed-b866-c73d72ca87d2",
   "metadata": {},
   "source": [
    "# Load Images Using ImageFolder\n",
    "\n",
    "In this step, we utilize the `ImageFolder` class from `torchvision.datasets` to efficiently load images from the defined directories:\n",
    "\n",
    "- **Training Dataset**:\n",
    "  - Loaded from `train_directory`.\n",
    "  - Applies training-specific transformations (e.g., data augmentation).\n",
    "\n",
    "- **Validation Dataset**:\n",
    "  - Loaded from `valid_directory`.\n",
    "  - Applies validation-specific transformations (e.g., normalization).\n",
    "\n",
    "This approach allows PyTorch to automatically categorize images based on the folder structure, with each subfolder representing a class, streamlining the data loading process for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ffabc-15e8-4e5b-9f6b-c2d0ed09d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folders\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b6ae2-25bf-4423-a0ce-d2985aac224d",
   "metadata": {},
   "source": [
    "# Dataset Sizes and Data Loaders\n",
    "\n",
    "- **Calculating Dataset Sizes**:\n",
    "  - Use `len()` to compute the number of images in both the training and validation datasets.\n",
    "  - Provides an overview of the available data for each phase.\n",
    "\n",
    "- **Creating Data Loaders**:\n",
    "  - Utilize `DataLoader` from `torch.utils.data` for both training and validation datasets.\n",
    "  - Handles batching, shuffling, and multi-threaded loading to efficiently supply data to the model.\n",
    "\n",
    "### Data Loader Parameters:\n",
    "- **Batch Size (bs)**:\n",
    "  - Controls the number of samples processed per iteration.\n",
    "  \n",
    "- **Shuffling**:\n",
    "  - Randomly shuffles data at each epoch to improve model robustness.\n",
    "\n",
    "- **Number of Workers (num_cpu)**:\n",
    "  - Sets the number of CPU threads for data loading, enhancing efficiency.\n",
    "\n",
    "- **Pin Memory**:\n",
    "  - Speeds up data transfer to the GPU, improving training performance.\n",
    "\n",
    "- **Drop Last**:\n",
    "  - Discards the last incomplete batch if the dataset size isn’t divisible by the batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379d1db-70e6-418e-ba31-08abdb200e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train':len(dataset['train']),\n",
    "    'valid':len(dataset['valid'])\n",
    "}\n",
    "\n",
    "# Create iterators for data loading\n",
    "dataloaders = {\n",
    "    'train':data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=0, pin_memory=True, drop_last=True),\n",
    "    'valid':data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=0, pin_memory=True, drop_last=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64166b76-2fc2-4bc8-af67-fb4ecd5a2a93",
   "metadata": {},
   "source": [
    "# Display Sample Images\n",
    "\n",
    "In this section, we visualize a selection of sample images from the training dataset to ensure correct loading and transformation application. This step aids in understanding the data's distribution and quality.\n",
    "\n",
    "- **Purpose**:\n",
    "  - Confirm that images are loaded correctly.\n",
    "  - Validate the application of transformations.\n",
    "  - Provide insights into the dataset's distribution and quality.\n",
    "\n",
    "- **Visualization Tool**:\n",
    "  - Typically utilize `matplotlib` for image display.\n",
    "  \n",
    "- **Display Format**:\n",
    "  - Retrieve images from the training data loader.\n",
    "  - Plot images in a grid format for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c5a3f-dfeb-487e-bc0d-0996d87774cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize images\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, labels = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# Plot images\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(denormalize(out).squeeze(0).permute(1, 2, 0))  # Add squeeze(0) before permute\n",
    "plt.axis('off')\n",
    "plt.title('Sample Images from Training Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Print labels\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b369e78-ae72-4b39-b240-9651c87d9998",
   "metadata": {},
   "source": [
    "# Class Names or Target Labels\n",
    "\n",
    "In this section, we retrieve and display the class names (target labels) from the training dataset. These class names correspond to the subfolder names in the `train_directory`, helping to identify the different categories of images used for training and validation.\n",
    "\n",
    "- **Retrieve Class Names**:\n",
    "  - Extract class names from the subfolder structure in the `train_directory`.\n",
    "\n",
    "- **Print Dataset Sizes**:\n",
    "  - Display the sizes of both the training and validation datasets.\n",
    "  - Provides an overview of data distribution, which is crucial for understanding the dataset scale and ensuring it meets the training model's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cfb82-3ed2-4a96-b74c-cce260d7c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    " \n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\",dataset_sizes['train'],\n",
    "      \"\\nValidation-set size:\", dataset_sizes['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3088c-6183-49e8-a98e-16e1dad81dc0",
   "metadata": {},
   "source": [
    "# Set Default Device as GPU, If Available\n",
    "\n",
    "In this section, we check for GPU availability for computation and set the device accordingly. Utilizing a GPU can significantly enhance the speed of training and inference processes in deep learning models.\n",
    "\n",
    "- **Device Check**:\n",
    "  - Determine if a GPU is available using PyTorch's built-in functionality.\n",
    "  \n",
    "- **Set Device**:\n",
    "  - If a GPU is available, set the device to GPU.\n",
    "  - If no GPU is found, fall back to using the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f095ef9-f011-46ee-9445-70f02753d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b3027-fd7a-434c-a551-041d3eb07fa9",
   "metadata": {},
   "source": [
    "# Loading a Pretrained Model for Fine-Tuning\n",
    "\n",
    "In this section, we load the ResNet-50 model pretrained on ImageNet, allowing us to leverage learned features from a large dataset. This approach can significantly enhance performance, especially with limited training data.\n",
    "\n",
    "- **Loading the Model**:\n",
    "  - Utilize `torchvision.models` to load the ResNet-50 architecture.\n",
    "  - Set `pretrained=True` to initialize the model with weights learned from the ImageNet dataset.\n",
    "\n",
    "- **Modifying the Final Layer**:\n",
    "  - If `RETURN_PREACTIVATION` is set to True:\n",
    "    - Remove the final fully connected layer to output features directly, useful for specific tasks.\n",
    "  - Otherwise:\n",
    "    - Replace the final layer with a new fully connected layer matching the number of output features to the number of classes in our dataset.\n",
    "\n",
    "- **Transferring the Model to GPU**:\n",
    "  - Move the model to the GPU (if available) using the previously set device.\n",
    "  - This enables efficient use of the GPU's computational power during training.\n",
    "\n",
    "By loading and modifying a pretrained model, we can effectively adapt it to our specific classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c079a2-2ab2-44cf-a520-790f4b817960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading resnet50 for finetuning ...\\n\")\n",
    "\n",
    "model = torchvision.models.__dict__['resnet50'](pretrained=True)\n",
    "if RETURN_PREACTIVATION:\n",
    "    model.fc = torch.nn.Sequential()\n",
    "else:\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model_ft = model.cuda()\n",
    "\n",
    "# Transfer the model to GPU\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8075c-ed6d-49bc-b084-36a4cedf8f57",
   "metadata": {},
   "source": [
    "# Printing the Model Summary\n",
    "\n",
    "In this section, we print a summary of the model, providing insights into its architecture, layer details, parameters, and gradient requirements for training.\n",
    "\n",
    "- **Enumerating Model Parameters**:\n",
    "  - Iterate over the model's named parameters using `model.named_parameters()`.\n",
    "  - This provides parameter names along with their corresponding tensors.\n",
    "  - The `requires_grad` attribute indicates whether the parameter will be updated during training (i.e., if gradients will be computed for it).\n",
    "\n",
    "- **Using the Summary Function**:\n",
    "  - The `summary` function from `torchsummary` offers a comprehensive overview of the model architecture.\n",
    "  - Displays information such as:\n",
    "    - Output shape of each layer\n",
    "    - Number of parameters per layer\n",
    "    - Total number of trainable parameters in the model\n",
    "  - The `input_size` argument specifies the shape of the input images that the model expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978aea2-bbb8-491f-97a7-7a146be15c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print('Model Summary:-\\n')\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad )\n",
    "summary(model_ft, input_size=(3, 224, 224))\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3aefc6-cab2-46c1-b099-7f46709f0268",
   "metadata": {},
   "source": [
    "# Defining the Loss Function\n",
    "\n",
    "In this section, we define the loss function used to evaluate the model's performance during training.\n",
    "\n",
    "- **Loss Function**:\n",
    "  - We use `nn.CrossEntropyLoss()`, which is standard for multi-class classification problems.\n",
    "  - This loss function combines `LogSoftmax` and `NLLLoss` into a single class.\n",
    "  - It is designed to work with raw, unnormalized scores (logits) from the model, making it well-suited for our use case with multiple classes to predict.\n",
    "\n",
    "- **Purpose**:\n",
    "  - The cross-entropy loss measures the difference between predicted probabilities (after applying softmax) and actual class labels.\n",
    "  - During training, the model aims to minimize this loss, leading to improved accuracy in the classification task.\n",
    "\n",
    "Defining an appropriate loss function is crucial for guiding the model's learning process and achieving better performance on the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69c590-6279-4b79-9903-fde8db75c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32016a6d-5260-4317-a938-1aae1ecc5fa9",
   "metadata": {},
   "source": [
    "# Defining the Optimizer\n",
    "\n",
    "In this section, we define the optimizer used to update the model parameters during training.\n",
    "\n",
    "- **Optimizer Selection**:\n",
    "  - We use `optim.SGD`, which stands for Stochastic Gradient Descent.\n",
    "  - It is a widely-used optimization algorithm that updates model parameters based on gradients computed from the loss function.\n",
    "\n",
    "- **Model Parameters**:\n",
    "  - `model_ft.parameters()` retrieves all the parameters of the model that need to be optimized.\n",
    "\n",
    "- **Learning Rate (lr)**:\n",
    "  - Set to 0.01, it controls how much to adjust the model parameters in response to estimated gradients.\n",
    "  - A smaller learning rate promotes stable convergence, while a larger one may speed up training but risks overshooting optimal values.\n",
    "\n",
    "- **Momentum**:\n",
    "  - Set to 0.9, momentum helps accelerate SGD in the relevant direction and dampens oscillations.\n",
    "  - It keeps track of past gradients to smooth out updates, facilitating faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c42d4d-5659-47f7-b3f3-10c2a6fda095",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b02bb-8207-440e-a89a-3a8d4f621bbc",
   "metadata": {},
   "source": [
    "# Defining the Learning Rate Scheduler\n",
    "\n",
    "In this section, we define a learning rate scheduler to adjust the learning rate during training.\n",
    "\n",
    "- **Scheduler Selection**:\n",
    "  - We use `lr_scheduler.StepLR`, which reduces the learning rate by a specified factor after a set number of epochs.\n",
    "  - This approach can enhance convergence, especially in the later stages of training.\n",
    "\n",
    "- **Optimizer**:\n",
    "  - `optimizer_ft` is passed to the scheduler, enabling it to adjust the learning rate used by this optimizer.\n",
    "\n",
    "- **Step Size (step_size)**:\n",
    "  - Set to 25, indicating that the learning rate will be decreased every 25 epochs.\n",
    "  - This helps stabilize the model as it approaches convergence.\n",
    "\n",
    "- **Decay Factor (gamma)**:\n",
    "  - Set to 0.1, which specifies that the learning rate will be multiplied by 0.1 after each step.\n",
    "  - This reduction allows for more refined updates to the model parameters as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c887a16-d55e-430a-8dee-fa933545a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5240b-5932-4962-b350-a17d6fb4f399",
   "metadata": {},
   "source": [
    "# Model Training Routine\n",
    "\n",
    "In this section, we define a function `train_model` that orchestrates the training and validation process for the deep learning model.\n",
    "\n",
    "## Function Definition\n",
    "- **Parameters**:\n",
    "  1. **model**: The neural network model to be trained.\n",
    "  2. **criterion**: The loss function used to evaluate the model's performance.\n",
    "  3. **optimizer**: The optimization algorithm to update the model's parameters.\n",
    "  4. **scheduler**: The learning rate scheduler to adjust the learning rate during training.\n",
    "  5. **num_epochs**: The total number of epochs for training.\n",
    "\n",
    "## Initialization\n",
    "- Initialize best model weights and best accuracy for comparison.\n",
    "- Create a TensorBoard `SummaryWriter` to log training metrics.\n",
    "\n",
    "## Training Loop\n",
    "- Iterate through each epoch, alternating between training and validation phases.\n",
    "- Set model to training mode (`model.train()`) for training and evaluation mode (`model.eval()`) for validation.\n",
    "\n",
    "## Data Iteration\n",
    "- Iterate through the data using the dataloaders.\n",
    "- Move inputs and labels to the appropriate device (CPU or GPU).\n",
    "- Zero optimizer gradients to prevent accumulation from previous iterations.\n",
    "\n",
    "## Forward Pass\n",
    "- Generate predictions with a forward pass.\n",
    "- Compute loss using the specified criterion.\n",
    "- In training mode, perform backpropagation (`loss.backward()`) and update model parameters (`optimizer.step()`).\n",
    "\n",
    "## Loss and Accuracy Calculation\n",
    "- Accumulate running loss and correct predictions to calculate epoch loss and accuracy.\n",
    "- Log training and validation metrics using TensorBoard for visual representation.\n",
    "\n",
    "## Best Model Checkpointing\n",
    "- Save model weights if the validation accuracy exceeds the best recorded accuracy.\n",
    "\n",
    "## Completion and Return\n",
    "- Print total training time and best validation accuracy achieved.\n",
    "- Load and return the best model weights for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a83d5-977c-4378-af3d-803abbd2d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Model training routine\n",
    "print(\"\\nTraining:-\\n\")\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Lists to store loss and accuracy values\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_accs.append(epoch_acc.item())\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "            else:\n",
    "                valid_losses.append(epoch_loss)\n",
    "                valid_accs.append(epoch_acc.item())\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Plotting the training and validation loss and accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(num_epochs), valid_losses, label='Valid Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(num_epochs), train_accs, label='Train Accuracy')\n",
    "    plt.plot(range(num_epochs), valid_accs, label='Valid Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b69998-1151-4b0f-9b1e-79f5bdddd060",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ec68d-3c02-4278-ba7c-6ac791ef3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ea746-f769-4150-b1bf-57879a400f83",
   "metadata": {},
   "source": [
    "# Paths for Evaluation Image Directory and Model Checkpoint\n",
    "\n",
    "In this section, we define the paths used for loading evaluation images and saving the model checkpoints during the training process.\n",
    "\n",
    "## Evaluation Image Directory\n",
    "- **Path**: Specify the directory where the evaluation images are stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72119f-950b-44a5-ad11-f1129e7badf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DIR = test_directory\n",
    "EVAL_MODEL = model_save_path\n",
    "print(EVAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c840c-6da1-4a2d-be8a-1f1734813fd5",
   "metadata": {},
   "source": [
    "# Load the model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c60d3e-290f-4492-b27a-6d035b34cf15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.load(EVAL_MODEL)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c5039-ba6d-4d0a-b2b6-8ec8b1f421f0",
   "metadata": {},
   "source": [
    "# Configure batch size and number of CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14896b06-685a-452c-b9ab-b6aaa4d57f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpu = multiprocessing.cpu_count()\n",
    "bs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eda045-0809-4e40-a0d6-802e63f907e9",
   "metadata": {},
   "source": [
    "# Prepare the eval data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b05fb-4561-43c8-973c-de5fbf2a3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(size=256),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "eval_dataset = datasets.ImageFolder(root=EVAL_DIR, transform=eval_transform)\n",
    "eval_loader = data.DataLoader(eval_dataset, batch_size=bs, shuffle=True,\n",
    "                              num_workers=num_cpu, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e074f1-607b-42cf-bbf0-18f9f97aaaba",
   "metadata": {},
   "source": [
    "# Enable GPU mode, if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb44542-8872-4a7a-973f-24a0ede3d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7f2dc-7d7e-4e13-a145-f09be5eb1af2",
   "metadata": {},
   "source": [
    "# Number of classes and dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a3eab-47c3-4370-9ff0-b7b8137eb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(eval_dataset.classes)\n",
    "dsize = len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4037c3-31bd-4808-9cfd-80a8218626bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c38a9-bcde-4698-b394-3c922397fc91",
   "metadata": {},
   "source": [
    "# Class label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae16df4-cc65-4946-afbc-e6e1d5f609ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_classes == 2:\n",
    "    class_names = ['Luminal', 'Non_Luminal']\n",
    "else:\n",
    "    class_names = ['LuminalA', 'LuminalB', 'HER2-enriched', 'triple_negative']\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcf424-b4fc-4243-a11e-ff50c602d7d3",
   "metadata": {},
   "source": [
    "# Initialize the prediction and label lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d5dca-6825-4aa2-92ae-92fd26988742",
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "lbllist = torch.zeros(0, dtype=torch.long, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f088c4e-edf1-48f1-9bb4-d371f896bc45",
   "metadata": {},
   "source": [
    "# Initilizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e5077-a859-4b73-a030-0cc3e932f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "predlist = torch.tensor([])  # to store the predicted labels\n",
    "lbllist = torch.tensor([])   # to store the true labels\n",
    "all_probs = torch.tensor([])  # to store probabilities for ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f0310-7f32-43bf-8fca-77eccbeba810",
   "metadata": {},
   "source": [
    "# Model Evaluation Mode\n",
    "\n",
    "In this section, we set the model to evaluation mode, which is essential for performing inference on the evaluation dataset. Switching to evaluation mode adjusts the behavior of certain layers in the model, such as dropout and batch normalization, ensuring that they function appropriately during evaluation.\n",
    "\n",
    "## Setting the Model to Evaluation Mode\n",
    "\n",
    "1. **Switching Mode:**\n",
    "   - The model's mode is changed to evaluation by calling the `.eval()` method. This informs the model that it is in inference mode, which is crucial for getting accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc4c88-bdd6-4f6b-8966-975cb9542838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()  # Ensure the model is in evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48267140-6679-492d-95ba-e9b5ff704fef",
   "metadata": {},
   "source": [
    "# Evaluate the model accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b587f99-619a-412c-a781-dfaf5b181047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in eval_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get probabilities using softmax\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs = torch.cat([all_probs, probs.cpu()])\n",
    "        \n",
    "        # Get the predicted class with the highest score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        predlist = torch.cat([predlist, predicted.view(-1).cpu()])\n",
    "        lbllist = torch.cat([lbllist, labels.view(-1).cpu()])\n",
    "\n",
    "# Convert lbllist and predlist to integers for bincount\n",
    "lbllist = lbllist.to(torch.int64)\n",
    "predlist = predlist.to(torch.int64)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = 100 * correct / total\n",
    "print('Accuracy of the network on the {:d} test images: {:.2f}%'.format(total, overall_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dddc37-3abc-49e3-abc8-0dc443c4b20e",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168ad66-820b-460b-b1e9-327205c48ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Convert tensors to numpy arrays if they're not already\n",
    "lbllist = lbllist.numpy()\n",
    "predlist = predlist.numpy()\n",
    "all_probs = all_probs.numpy()\n",
    "\n",
    "# Get the number of classes\n",
    "n_classes = len(eval_dataset.classes)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(lbllist, predlist, labels=np.arange(n_classes))\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(conf_mat,'\\n')\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=eval_dataset.classes, \n",
    "            yticklabels=eval_dataset.classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# One-hot encode the labels for ROC and PR curves\n",
    "lbllist_one_hot = np.eye(n_classes)[lbllist]\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(lbllist_one_hot[:, i], all_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], label=f'{eval_dataset.classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC for each class\n",
    "print(\"\\nAUC for each class:\")\n",
    "for i in range(n_classes):\n",
    "    print(f\"{eval_dataset.classes[i]}: {roc_auc[i]:.2f}\")\n",
    "\n",
    "# Precision-Recall Curve and Average Precision\n",
    "precision = {}\n",
    "recall = {}\n",
    "avg_precision = {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(lbllist_one_hot[:, i], all_probs[:, i])\n",
    "    avg_precision[i] = average_precision_score(lbllist_one_hot[:, i], all_probs[:, i])\n",
    "    plt.plot(recall[i], precision[i], label=f'{eval_dataset.classes[i]} (AP = {avg_precision[i]:.2f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print Average Precision for each class\n",
    "print(\"\\nAverage Precision for each class:\")\n",
    "for i in range(n_classes):\n",
    "    print(f\"{eval_dataset.classes[i]}: {avg_precision[i]:.2f}\")\n",
    "\n",
    "# Calculate and print overall metrics\n",
    "print(\"\\nOverall Metrics:\")\n",
    "print(f\"Accuracy: {np.mean(lbllist == predlist):.2f}\")\n",
    "print(f\"Macro AUC: {np.mean(list(roc_auc.values())):.2f}\")\n",
    "print(f\"Macro Average Precision: {np.mean(list(avg_precision.values())):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575ce3c-3321-4a1a-afb6-aacad2db415a",
   "metadata": {},
   "source": [
    "# Hook to Extract Gradients and Feature Maps\n",
    "\n",
    "In this section, we define a hook mechanism that allows us to capture gradients and feature maps from specific layers of the neural network. Hooks provide a powerful way to observe and analyze the inner workings of the model during both the forward and backward passes.\n",
    "\n",
    "## What Are Hooks?\n",
    "\n",
    "- **Definition:** Hooks are functions that can be registered to layers or operations within the neural network. They can be triggered during the forward or backward pass of the network, allowing us to access intermediate outputs or gradients.\n",
    "\n",
    "## Purpose of Using Hooks\n",
    "\n",
    "1. **Extract Feature Maps:**\n",
    "   - By attaching a hook to a specific layer, we can capture the output (feature map) of that layer during the forward pass. This enables us to visualize what features are being detected by the model at different stages of processing.\n",
    "\n",
    "2. **Capture Gradients:**\n",
    "   - Hooks can also be used to extract gradients during the backward pass. This allows us to analyze how the loss is influencing the parameters of different layers and understand the importance of specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d4a2d-525f-4f09-bf27-e082a88c3b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SaveFeatures:\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.features = None\n",
    "        self.gradients = None\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.hook_grad = module.register_backward_hook(self.hook_grad_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "\n",
    "    def hook_grad_fn(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]  # Gradient with respect to the output\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "        self.hook_grad.remove()\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        conv_output = SaveFeatures(self.target_layer)  # Hook to get feature maps and gradients\n",
    "        model_output = self.model(x)\n",
    "\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(model_output)  # Default to the class with the highest score\n",
    "\n",
    "        one_hot = torch.zeros((1, model_output.size()[-1]), dtype=torch.float32).to(x.device)\n",
    "        one_hot[0][class_idx] = 1\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)  # Backpropagate for the target class\n",
    "\n",
    "        # Get the gradients and feature maps from the hook\n",
    "        gradients = conv_output.gradients\n",
    "        feature_maps = conv_output.features[0]\n",
    "\n",
    "        # Perform global average pooling to get the weights\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        # Multiply feature maps by the pooled gradients\n",
    "        for i in range(len(pooled_gradients)):\n",
    "            feature_maps[i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # Average along the channel dimension to get the Grad-CAM heatmap\n",
    "        heatmap = torch.mean(feature_maps, dim=0).cpu().detach().numpy()\n",
    "\n",
    "        # Apply ReLU\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "        # Normalize the heatmap\n",
    "        heatmap /= np.max(heatmap)\n",
    "        return heatmap\n",
    "\n",
    "# Function to overlay Grad-CAM heatmap on the image\n",
    "def overlay_heatmap(heatmap, img):\n",
    "    # Resize heatmap to match image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Convert heatmap to uint8 format\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    \n",
    "    # Apply color map (e.g., JET) to the heatmap\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Convert the original image to uint8 (if it's float32)\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.uint8(255 * img)\n",
    "    \n",
    "    # Ensure the original image has 3 channels (if it's grayscale, convert to RGB)\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:  # If grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Overlay the heatmap onto the original image\n",
    "    overlayed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "    \n",
    "    return overlayed_img\n",
    "\n",
    "# Load your test image and preprocess\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = cv2.imread(image_path, 1)\n",
    "    image = np.float32(cv2.resize(image, (224, 224))) / 255\n",
    "    img_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return img_tensor, image\n",
    "\n",
    "# Function to process all images in a folder\n",
    "def process_images_in_folder(folder_path, transform, model, target_layer):\n",
    "    grad_cam = GradCam(model, target_layer)\n",
    "    \n",
    "    # Create a results folder if it doesn't exist\n",
    "    results_folder = os.path.join(root, 'gradcam_results')\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all images in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg'):  # Add other formats if necessary\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image_tensor, original_img = preprocess_image(image_path, transform)\n",
    "            \n",
    "            # Run Grad-CAM\n",
    "            heatmap = grad_cam(image_tensor.to(device))\n",
    "            overlayed_image = overlay_heatmap(heatmap, original_img)\n",
    "            \n",
    "            # Save the overlayed image\n",
    "            result_path = os.path.join(results_folder, f'gradcam_{filename}')\n",
    "            cv2.imwrite(result_path, overlayed_image)\n",
    "\n",
    "            # Optional: Display the result\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(original_img)\n",
    "            plt.title('Original Image')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(overlayed_image)\n",
    "            plt.title('Grad-CAM Heatmap')\n",
    "            plt.show()\n",
    "\n",
    "# Example usage of Grad-CAM on all images in a folder\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Replace with the layer you want to visualize (e.g., last conv layer in ResNet)\n",
    "target_layer = model.layer4[-1]  # Last convolutional layer in ResNet\n",
    "\n",
    "# Specify the folder containing the test images\n",
    "\n",
    "\n",
    "if num_classes == 2:\n",
    "    folder_path = EVAL_DIR + '/Luminal'\n",
    "else:\n",
    "    folder_path = EVAL_DIR + '/LuminalA'\n",
    "\n",
    "print(folder_path)\n",
    "\n",
    "# Process all images in the specified folder\n",
    "process_images_in_folder(folder_path, transform, model, target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894c2bf-b895-46d1-8b35-fb19a70d1e03",
   "metadata": {},
   "source": [
    "# Function to Extract Features from a Target Layer\n",
    "\n",
    "In this section, we define a function specifically designed to extract feature maps from a designated layer of the neural network. This functionality allows us to analyze the representations learned at various stages of the network and provides insights into how the model interprets input data.\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "The primary goal of this function is to capture the output (feature map) from a particular layer during the forward pass of the model. This is particularly valuable for:\n",
    "\n",
    "1. **Visualizing Learned Representations:**\n",
    "   - By examining feature maps, we can observe how different types of features are detected by the model. For example, we can identify low-level features such as edges and textures in the early layers, and higher-level features like objects and shapes in the deeper layers.\n",
    "\n",
    "2. **Understanding Network Processing:**\n",
    "   - Analyzing feature maps helps us understand how the network processes input data at various depths, revealing the hierarchy of features the model learns throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f9c1f-ad5a-4e09-a65c-347b0def8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, data_loader, target_layer):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Hook to get the feature maps from the specified layer\n",
    "    class SaveFeatures:\n",
    "        def __init__(self, module):\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "            self.features = None\n",
    "\n",
    "        def hook_fn(self, module, input, output):\n",
    "            self.features = output\n",
    "\n",
    "        def close(self):\n",
    "            self.hook.remove()\n",
    "\n",
    "    # Attach hook to the target layer\n",
    "    conv_output = SaveFeatures(target_layer)\n",
    "    \n",
    "    # Loop through the dataset\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            features = conv_output.features.cpu().detach().numpy()\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    conv_output.close()\n",
    "    \n",
    "    # Stack all features and labels together\n",
    "    all_features = np.vstack(all_features)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    \n",
    "    return all_features, all_labels\n",
    "\n",
    "# Function to generate t-SNE plot\n",
    "def generate_tsne_plot(features, labels):\n",
    "    # Flatten the feature maps if needed (depending on the layer you extract from)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    \n",
    "    # Apply t-SNE to reduce dimensionality to 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_tsne = tsne.fit_transform(features)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Define colors and labels\n",
    "    if num_classes == 2:\n",
    "        colors = ['blue', 'red']\n",
    "        class_labels = ['Luminal', 'Non_Luminal']\n",
    "    else:\n",
    "        colors = ['blue', 'red', 'green', 'pink']\n",
    "        class_labels = ['LuminalA', 'LuminalB', 'HER2-enriched', 'triple_negative'] \n",
    "    \n",
    "    # Plot each class separately for visualization\n",
    "    for class_idx in range(num_classes):\n",
    "        indices = np.where(labels == class_idx)\n",
    "        plt.scatter(features_tsne[indices, 0], features_tsne[indices, 1], \n",
    "                    c=colors[class_idx], label=class_labels[class_idx])\n",
    "    \n",
    "    plt.title('t-SNE plot of Feature Maps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# target_layer = model.layer4[-1]  # Example for ResNet last conv layer\n",
    "all_features, all_labels = extract_features(model, eval_loader, target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3ff54-ab2e-4d0e-b2c7-19d2b46074a4",
   "metadata": {},
   "source": [
    "# Generate and plot the t-SNE result\n",
    "\n",
    "Extract Features: A function is used to extract high-dimensional feature representations from a specific layer of the model. This helps to visualize what the model learns at different stages.\n",
    "\n",
    "Apply t-SNE: The t-SNE algorithm reduces the dimensionality of the extracted features from potentially hundreds or thousands of dimensions to just 2 or 3 dimensions, making it easier to visualize patterns in the data.\n",
    "\n",
    "Plot t-SNE: The resulting 2D or 3D t-SNE plot shows how the model's features cluster, and we color-code these clusters by class (e.g., Luminal vs Non_Luminal) to see how well the model distinguishes between them visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb816a-7651-49a3-8de9-7fb21571e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tsne_plot(all_features, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20761746-7725-4634-8458-1c26e89cab7b",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
